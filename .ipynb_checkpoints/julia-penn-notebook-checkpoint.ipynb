{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:131e8af0686ad4a3d7b23f31a9285764cb6818c86b6aaf66ebdfe168b5733c2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img style=\"float: left\" src=\"julia.png\">\n",
      "                     \n",
      "## An Introduction To Machine Learning Using Julia\n",
      "<br><br><br>\n",
      "November 13, 2014<br>\n",
      "Wu and Chen Auditorium<br>\n",
      "University of Pennsylvania<br><br>\n",
      "Presenter: Randy Zwitch <br>\n",
      "Twitter: @randyzwitch<br>\n",
      "Website: http://randyzwitch.com\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Why Use Julia For Machine Learning?\n",
      "<br><br>\n",
      "\u201cJulia is a high-level, high-performance dynamic programming language for technical computing, with syntax that is familiar to users of other technical computing environments. It provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library. The library, largely written in Julia itself, also integrates mature, best-of-breed C and Fortran libraries for linear algebra, random number generation, signal processing, and string processing.\n",
      "\n",
      "Julia programs are organized around multiple dispatch; by defining functions and overloading them for different combinations of argument types, which can also be user-defined.\u201d \n",
      "\n",
      "   [-julialang.org](http://julialang.org)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Compiled Languages = Fast \n",
      "##Just-in-Time Compilation = Interactive <br><br><br>\n",
      "\n",
      "![](benchmarks.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Today's Agenda<br>  \n",
      "\n",
      "###Using Julia, demonstrate:<br>\n",
      "* Linear Regression\n",
      "    * Normal Equation \n",
      "    * Gradient Descent\n",
      "<br><br>      \n",
      "* k-Means Clustering - [Clustering.jl](https://github.com/JuliaStats/Clustering.jl)\n",
      "<br><br>   \n",
      "* Random Forest - [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_number": 5,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Linear Regression (and Generalized Linear Models overall) are the workhorses of predictive modeling. While considered by some as \"old\" methods or solely the domain of statisticians, deeply understanding the intuition AND the implementation of Linear Regression is the key to moving into the more \"sexy\" machine learning algorithms that make up modern Machine Learning. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 5,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Linear Regression - Normal Equation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 7,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Linear Regression models are most often estimated by the \"Least Squares\" method and take the functional form of: <br>\n",
      "\n",
      "${Y_i}=\\beta_0 +\\beta_1 X_i+\\dots\\beta_p X_{ip}+\\epsilon_i$  = $X^T\\beta+\\epsilon_i$ \n",
      "<br><br>for i=1, $\\dots$ , n observations and p parameters <br><br>\n",
      "\n",
      "Note: $^T$denotes a transposed matrix and $X^T\\beta$ is the inner product between vectors $x_i$ and $\\beta$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 7,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Solving for $\\hat{\\beta}$ <br>\n",
      "\n",
      "In the prior equation, $\\beta$ is the set of parameters that minimize the sum-of-squared residuals ($\\sum_{i=1}^n(y - \\hat{y})^2$). It can be shown through substitution that we can estimate $\\beta$ (written $\\hat{\\beta}$ and spoken \"Beta-hat\") as: <br><br>\n",
      "\n",
      "$\\hat{\\beta}$ = $(X^TX)^{-1}(X^Ty)$ <br><br>\n",
      "\n",
      "Luckily, due to the focus on linear algebra from the earliest days of Julia, the syntax for solving this equation is straight-forward.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 7,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##LIVE DEMO 1: Estimating Car Miles-Per-Gallon from Engine Displacement"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load publicly available datasets from R, charting library\n",
      "using RDatasets, Gadfly"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 10
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load the data\n",
      "mtcars = dataset(\"datasets\", \"mtcars\")\n",
      "\n",
      "#Describe the dataset using the `describe` function on the mtcars DataFrame\n",
      "describe(mtcars[[:Disp, :MPG]]) "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 11,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Plot the data\n",
      "set_default_plot_size(20cm, 12cm)\n",
      "plot(mtcars, x=\"Disp\", y=\"MPG\", \n",
      "     Geom.point, \n",
      "     Guide.title(\"Miles Per Gallon vs. Engine Displacement\"),\n",
      "     Theme(default_color = color(\"navy\")))"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 11,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Solving for parameters using linear algebra\n",
      "#Can use unicode to represent objects, making math feel more natural  \n",
      "\n",
      "#Call array() on data frame column to get data into \n",
      "#native Julia arrays\n",
      "y = array(mtcars[:MPG])\n",
      "x = array(mtcars[:Disp])\n",
      "X = hcat(ones(Float64, size(x)[1]), x) #Add a column of ones to represent intercept term\n",
      "\n",
      "#Use \"beta\" unicode value to represent the parameters\n",
      "\u03b2 = inv((X'X))*(X'y)\n",
      "\n",
      "#Print intercept and slope\n",
      "println(\u03b2)"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 11,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Score dataset: Since \u03b2 and X are compatible dimension arrays, \n",
      "#we can take inner product to get our predicted values\n",
      "pred = X*\u03b2\n",
      "\n",
      "#Plot data and linear regression line\n",
      "set_default_plot_size(20cm, 12cm)\n",
      "plot(Guide.title(\"Miles Per Gallon vs. Engine Displacement with Regression Line\"),\n",
      "    layer(mtcars, x=\"Disp\", y=\"MPG\", \n",
      "          Geom.point, Theme(default_color = color(\"navy\"))), \n",
      "    layer(x=mtcars[:Disp], y=pred, \n",
      "          Geom.line, Theme(default_color = color(\"red\")))\n",
      ") "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 11,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 11,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 16
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "If there exists an <i>exact</i> method to solve linear models, why are there other techniques for estimating $\\beta$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 17,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Inverting matrices is computationally EXPENSIVE!\n",
      "* Data may not fit in memory\n",
      "* Needing to wait until all of your data has been collected = SLOW! An approximated solution <u>NOW</u> is better than a perfect solution <u>LATER</u>. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 17,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      " ##Batch & Stochastic Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 19,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* <b>Batch Gradient Descent</b>\n",
      "    * Requires having all data collected <u>before</u> starting (RAM intensive)\n",
      "    * Iterative algorithm that minimizes a cost function $J(\\theta)$ by moving $\\theta$ parameters in direction of steepest decrease in calculated gradient\n",
      "    * Depending on value of $\\alpha$ (learning rate), the algorithm will converge faster or slower (or not at all)\n",
      "<br><br>\n",
      "    \n",
      "* <b>Stochastic/Online Gradient Descent</b>\n",
      "    * Unlike Batch Gradient Descent, gradient calculated on a single sample of data, then $\\theta$ parameters updated\n",
      "    * As samples stream in, assumption is that all samples generated from same distribution\n",
      "    * SGD can often reach a local minimum faster than batch gradient descent, at the (potential) cost of not reaching the 'truer' minimum that batch gradient descent would provide\n",
      "    * Very Memory efficient (1 point/sample at a time in RAM)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Batch Gradient Descent Example\n",
      "y = array(mtcars[:MPG])\n",
      "x = array(mtcars[:Disp])\n",
      "\n",
      "#Scale X using [0,1] method\n",
      "#This will make minimum of function easier to find\n",
      "minx = minimum(x,1)\n",
      "maxx = maximum(x,1)\n",
      "x_scaled = (x .- minx) ./(maxx .- minx)\n",
      "\n",
      "#Add a column of ones to represent intercept term\n",
      "X = hcat(ones(Float64, size(x_scaled)[1]), x_scaled)\n",
      "\n",
      "#Learning rate and number of iterations\n",
      "\u03b1 =  0.1\n",
      "num_iters = 100000\n",
      "\n",
      "#Cost function J(\u03b8) = [(X * \u03b8 - Y)^2] / (2n)\n",
      "function cost(X::Array, Y::Array, \u03b8::Array)\n",
      "    return sum( (X * \u03b8 - y).^2 ) / (2 * size(y)[1])\n",
      "end\n",
      "\n",
      "#Initialize empty arrays to save results\n",
      "cost_history = Any[]\n",
      "theta_history =  Any[]\n",
      "\n",
      "#Initialize array for parameters\n",
      "\u03b8 = zeros(Float64, 2)\n",
      "\n",
      "for i in 1:num_iters\n",
      "  error =  (X * \u03b8 - y)\n",
      "  \u03b4 = (X' * error) / size(y)[1]\n",
      "  \u03b8 = \u03b8 - \u03b1 * \u03b4 \n",
      "  #Record intermediate results - Not strictly part of algorithm\n",
      "  push!(cost_history, cost(X, y, \u03b8))\n",
      "  push!(theta_history, \u03b8)\n",
      "end\n",
      "#Print result\n",
      "\u03b8 "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 19,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Score dataset: Since \u03b2 and X are compatible dimension arrays, \n",
      "#we can take inner product to get our predicted values\n",
      "pred2 = X*\u03b8\n",
      "\n",
      "#Plot data and linear regression line\n",
      "set_default_plot_size(20cm, 12cm)\n",
      "plot(Guide.title(\"Miles Per Gallon vs. Engine Displacement with Regression Line\"),\n",
      "    layer(mtcars, x=\"Disp\", y=\"MPG\", \n",
      "          Geom.point, Theme(default_color = color(\"navy\"))), \n",
      "    layer(x=mtcars[:Disp], y=pred, \n",
      "          Geom.line, Theme(default_color = color(\"red\"))),\n",
      "    layer(x=mtcars[:Disp], y=pred2, \n",
      "          Geom.line, Theme(default_color = color(\"green\")))\n",
      ") "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 19,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 19,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##For more in-depth information about Gradient Descent:\n",
      "\n",
      "Talk by John Myles White covering intuition and implementation of gradient descent algorithms (1 hr):\n",
      "\n",
      "http://www.hakkalabs.co/articles/streaming-data-analysis-and-online-learning/#!\n",
      "\n",
      "JuliaOpt Organization: Collection of packages for numerical optimization\n",
      "\n",
      "http://www.juliaopt.org/ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 19,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## k-means Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 24,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The k-means algorithm is an iterative algorithm that attempts to partition n observations into k clusters, where each point belongs to the cluster that it is closest to by Euclidean distance. The cluster center is defined as the mean of the points belonging to the cluster.\n",
      "\n",
      "Unlike linear regression, which has an explicit Y = X$\\beta$ + $\\epsilon$ form (\"Supervised Learning\"), the k-means algorithm has no Y \"target\" variable. Rather, k-means is an example of \"Unsupervised Learning\", understanding the structure of an underlying dataset rather than predicting a value from data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 24,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"kmeans-wikipedia.png\">\n",
      "<br><br>\n",
      "Source: http://en.wikipedia.org/wiki/K-means_clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 24,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##LIVE DEMO 2: Using Clustering.jl"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import Clustering package (RDatasets should already be loaded)\n",
      "using Clustering, RDatasets, DataFrames  "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 27
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import cars dataset again\n",
      "mtcars = dataset(\"datasets\", \"mtcars\")\n",
      "\n",
      "#kmeans function requires rows as features instead of columns; \n",
      "#transpose and convert to Float64\n",
      "mtcarsarray = array(mtcars[2:end])'\n",
      "mtcarsarray = convert(Array{Float64}, mtcarsarray)"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 28,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# performs K-means over mtcars numeric values, trying to group them into 5 clusters\n",
      "# set maximum number of iterations to 200\n",
      "# set display to :iter, so it shows progressive info at each iteration\n",
      "R = kmeans(convert(Array{Float64}, mtcarsarray), 10; \n",
      "           maxiter=200, display=:iter); "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 28,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# obtain the number of samples in each cluster\n",
      " # c[k] is the number of samples assigned to the k-th cluster\n",
      "c = R.counts'"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 30,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the centers (i.e. mean vectors)\n",
      "# M is a matrix of size (5, 20)\n",
      "# M[:,k] is the mean vector of the k-th cluster\n",
      "M = convert(DataFrame, R.centers')\n",
      "names!(M, names(mtcars[2:end]))\n",
      "\n",
      "#Print centers for selected attributes\n",
      "M[1:4]            "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 30,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 30,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Determining the Appropriate Number of Clusters\n",
      "\n",
      "In the prior example, there were several clusters where only one example was included. This is generally not desirable, since the point is to \"cluster\" like observations together. One method to remove the single-observation clusters is to specify fewer clusters at the beginning of the algorithm; another method would be to obtain more data points.\n",
      "\n",
      "How to determine the appropriate number clusters is beyond the scope of this talk, but include:\n",
      "* Visual inspection of the data - generally difficult to impossible\n",
      "* \"Elbow Method\" \n",
      "* Calculating \"Silhoutte\" statistic\n",
      "* Combine like clusters based on cosine simularity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 30,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Random Forest"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 34,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\"Random Forest is a trademark term for an ensemble of decision trees.\n",
      "\n",
      "Unlike single decision trees which are likely to suffer from high Variance or high Bias (depending on how they are tuned). Random Forests use averaging to find a natural balance between the two extremes.\n",
      "\n",
      "Since they have very few parameters to tune and can be used quite efficiently with default parameter settings (i.e. they are effectively non-parametric) Random Forests are good to use as a first cut when you don't know the underlying model, or when you need to produce a decent model under severe time pressure.\n",
      "\n",
      "This ease of use also makes Random Forests an ideal tool for people without a background in statistics, allowing lay people to produce fairly strong predictions free from many common mistakes, with only a small amount of research and programming.\"\n",
      "\n",
      "Source: https://www.kaggle.com/wiki/RandomForests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 34,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## LIVE DEMO 3: Using DecisionTree.jl"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using DecisionTree"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 36
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The \"iris\" dataset is pretty famous as an example\n",
      "# 4 features of 3 species of Iris flowers were collected by Sir Ronald Fisher (1936)\n",
      "# 50 exampled of each species are provided\n",
      "iris = dataset(\"datasets\", \"iris\")"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 37,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 37,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      " ##Sepal What? Petal Who?\n",
      "\n",
      "###Iris Versicolor\n",
      "<img src=\"Iris_versicolor_3.jpg\"> <br><br>\n",
      "\n",
      "Source: http://en.wikipedia.org/wiki/Iris_flower_data_set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First 4 colums are our features or \"predictor\" variables\n",
      "features = array(iris[:, 1:4]);\n",
      "#The label is what we are trying to predict: setosa, versicolor, virginica \n",
      "labels = array(iris[:, 5]);"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 37,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train random forest classifier\n",
      "# using 2 random features, 100 trees, and 0.5 portion of samples per tree (optional)\n",
      "model = build_forest(labels, features, 2, 100, 0.5)     "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 40
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# apply learned model\n",
      "# SepalLength, SepalWidth, PetalLength, PetalWidth\n",
      "apply_forest(model, [5.9, 3.0, 5.1, 1.9])"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 41
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run n-fold cross validation for forests to evaluate stability/performance\n",
      "# using 2 random features, 100 trees, 3 folds and 0.5 of samples per tree (optional)\n",
      "accuracy = nfoldCV_forest(labels, features, 2, 100, 3, 0.5);"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 42,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 42,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## End of Presentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 42,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## For more information about the Julia Language\n",
      "\n",
      "Main website:<br>\n",
      "http://julialang.org/\n",
      "\n",
      "Julia Users Google Group:<br>\n",
      "https://groups.google.com/forum/#!forum/julia-users\n",
      "\n",
      "Julia Developers Google Group:<br>\n",
      "https://groups.google.com/forum/#!forum/julia-dev\n",
      "\n",
      "JuliaBloggers - Aggregated Blog Posts about Julia<br>\n",
      "http://juliabloggers.com\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}